---
title: "Movie Recommender System using Baseline Predictors"
author: "Saloua Daouki"
date: "2025-05-31"
output: 
  pdf_document:
    toc: true
    number_sections: true
---

**Update Note for Resubmission (June 7, 2025):**

Per your feedback, the following clarifications and improvements were added to this submission:

1. **Explanation of the baseline recommender model** (under "Understanding the Baseline Model")
2. **Clarification of user and item biases**
3. **Discussion of model improvements, including regularization** (as a part of the conclusion)
4. **Definition and interpretation of RMSE** (under "RMSE Interpretation")

Thank you for your guidance!


```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This project builds a simple movie recommender system using the MovieLens dataset. I implement baseline predictors, which adjust predictions based on user and item effects relative to the global average rating. This model is a common starting point in collaborative filtering systems.

## Understanding the Baseline Model

The **baseline recommender** estimates a user's rating for an item by starting with the **global average rating** and adjusting it using two bias terms:

- **User bias**: how much a particular user tends to rate higher or lower than average.
- **Item bias**: how much a particular movie tends to receive higher or lower ratings than average.

The **core idea** is that some users consistently rate things higher (or lower), and some movies are universally liked or disliked. Capturing these effects improves prediction accuracy over just using a global average.

Formally, the baseline prediction formula is:

$$
\hat{r}_{ui} = \mu + b_u + b_i
$$

Where:

- \( \mu \) is the global average rating

- \( b_u \) is the bias for user \( u \)

- \( b_i \) is the bias for item \( i \)

This approach helps account for systematic rating patterns before applying more complex models.

## Dataset Description

I use the MovieLens 100k dataset, which includes userId, movieId, and rating. Ratings range from 1 to 5 and are sparse across users and items.

## Data Preparation

The dataset was loaded into R and split into training and test sets. A small subset of the data was used to verify calculations by hand. All analyses were performed in tidyverse.

```{r}
library(googledrive)
library(readr)
library(tidyverse)

# Specify the Drive file ID
file_id <- "1hwPfRD8x7QBQE5Vv4QyR7SLehfm-UH9d"

# Download the file to a temp location
temp_file <- tempfile(fileext = ".csv")
drive_download(as_id(file_id), path = temp_file, overwrite = TRUE)

# Read the CSV from temp file
ratings <- read_csv(temp_file)

head(ratings)
```

```{r}
set.seed(42)

# Add a random flag for train/test split
ratings_split <- ratings %>%
  group_by(userId) %>%
  mutate(split = sample(c("train", "test"), n(), replace = TRUE, prob = c(0.8, 0.2))) %>%
  ungroup()

train <- ratings_split %>% filter(split == "train")
test <- ratings_split %>% filter(split == "test")

head(ratings_split)
head(train)
head(test)
```

## Global Average Rating

The global average rating from the training data is:

```{r}
global_avg <- mean(train$rating, na.rm = TRUE)
print(global_avg)
```

Using this as a predictor for all unknown ratings, I calculated the RMSE on the test set:

```{r}
library(Metrics)

# Predict global average for all test ratings
test$pred_global <- global_avg

# Compute RMSE
rmse_global <- rmse(test$rating, test$pred_global)
print(paste("Global Average RMSE:", round(rmse_global, 4)))
```

## Baseline Predictor

I calculated user and item biases based on deviations from the global average. These were merged with the test set, and the baseline predictor was calculated as:

$$
Global Avg + User Bias + Item Bias
$$

```{r}
# User bias = avg user rating - global average
user_bias <- train %>%
  group_by(userId) %>%
  summarise(user_bias = mean(rating) - global_avg)

# Item bias = avg item rating - global average
item_bias <- train %>%
  group_by(movieId) %>%
  summarise(item_bias = mean(rating) - global_avg)

head(user_bias)
head(item_bias)
```

```{r}
# Merge user and item bias into test set
test <- test %>%
  left_join(user_bias, by = "userId") %>%
  left_join(item_bias, by = "movieId")

# Replace missing biases with 0 (for cold-start users/items)
test$user_bias[is.na(test$user_bias)] <- 0
test$item_bias[is.na(test$item_bias)] <- 0

# Predict using baseline
test <- test %>%
  mutate(pred_baseline = global_avg + user_bias + item_bias)

# Cap predictions to valid rating range (e.g., 1 to 5)
test <- test %>%
  mutate(pred_baseline = pmin(5, pmax(1, pred_baseline)))

head(test)
```

## RMSE for Baseline Predictor

After applying this model:

```{r}
rmse_baseline <- rmse(test$rating, test$pred_baseline)
print(paste("Baseline Predictor RMSE:", round(rmse_baseline, 4)))
```

This shows a significant improvement over the global average model.

## RMSE Interpretation

The **Root Mean Squared Error (RMSE)** measures the average prediction error of the recommender system:

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (r_i - \hat{r}_i)^2}
$$

Lower RMSE means better predictive performance. For example:

- A high RMSE (e.g., >1.0) suggests large discrepancies between predicted and actual ratings.
- A lower RMSE (e.g., 0.8 or below) suggests more accurate predictions.

In this project, the **baseline model** significantly reduced RMSE compared to the global average, meaning it captured meaningful structure in the data.


## Summarize Results

```{r}
results <- tibble(
  Model = c("Global Average", "Baseline Predictor"),
  RMSE = c(round(rmse_global, 4), round(rmse_baseline, 4))
)
print(results)
```

The baseline predictor significantly reduces error by accounting for individual user and item biases. This result supports the effectiveness of incorporating basic personalization into recommender systems.

## Conclusion

This analysis shows how a simple baseline recommender model can meaningfully outperform naive predictors by accounting for user and item effects. In practice, such models are useful starting points before applying more complex collaborative filtering or matrix factorization techniques.

A key enhancement to this model is **regularization**. In my current implementation, user and item biases are calculated as simple averages. However, users or movies with very few ratings can skew the bias estimates.

Regularization helps prevent **overfitting** by shrinking large biases when there's limited data. 

Let:

- \( I_u \): the set of items rated by user \( u \)

- \( U_i \): the set of users who rated item \( i \)

- \( \mu \): the global average rating

- \( \lambda \): the regularization parameter

The **regularized user bias** is given by:

$$
b_u = \frac{\sum_{i \in I_u} (r_{ui} - \mu - b_i)}{\lambda + |I_u|}
$$

Similarly, the **item bias** is:

$$
b_i = \frac{\sum_{u \in U_i} (r_{ui} - \mu - b_u)}{\lambda + |U_i|}
$$

These formulas are derived by minimizing a regularized squared error loss function, balancing model fit with complexity.

> *Note:* This formulation follows Koren & Bell (2009), who use \( R(u) \) in place of \( I_u \) and \( R(i) \) instead of \( U_i \).

### Reference  
Koren, Y., & Bell, R. (2009). *Advances in Collaborative Filtering*. In F. Ricci, L. Rokach, B. Shapira, & P. Kantor (Eds.), **Recommender Systems Handbook**. Springer.  
[Available online via DataJobs](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)

Other improvements include:

- Matrix factorization (e.g., SVD)

- Implicit feedback (e.g., views, clicks)

- Context-aware models


**Note:** All code used for data processing and analysis is available in this GitHub repository: [Project1](https://raw.githubusercontent.com/SalouaDaouki/Data612Summer25/refs/heads/main/Project1.Rmd?token=GHSAT0AAAAAADE5NEAG22TMZ4LL5INSLD5O2CE3MOA){.uri}
